{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "RNN_Lab.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djcoRfkyu9iW",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-Speech Tagging with Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_V3Ukdfu9iZ",
        "colab_type": "text"
      },
      "source": [
        "Your task in this assignment is to implement a simple part-of-speech tagger based on recurrent neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRaWsIW8u9ia",
        "colab_type": "text"
      },
      "source": [
        "## Problem specification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpeSWcznu9ic",
        "colab_type": "text"
      },
      "source": [
        "Your task in this assignment is\n",
        "\n",
        "1. to build a part-of-speech tagger based on a recurrent neural network architecture\n",
        "2. to train this tagger on the provided training data and identify a good model\n",
        "2. to evaluate the performance of this model on the provided test data\n",
        "\n",
        "To identify a good model, you can use the provided development (validation) data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVjBY0hsu9id",
        "colab_type": "text"
      },
      "source": [
        "## Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T8-KYeWu9ie",
        "colab_type": "text"
      },
      "source": [
        "Part-of-speech (POS) tagging is the task of labelling words (tokens) with [parts of speech](https://en.wikipedia.org/wiki/Part_of_speech). To give an example, consider the  sentence *Parker hates parsnips*. In this sentence, the word *Parker* should be labelled as a proper noun (a noun that is the name of a person), *hates* should be labelled as a verb, and *parsnips* should be labelled as a (common) noun. Part-of-speech tagging is an essential ingredient of many state-of-the-art natural language understanding systems.\n",
        "\n",
        "Part-of-speech tagging can be cast as a supervised machine learning problem where the gold-standard data consists of sentences whose words have been manually annotated with parts of speech. For the present assignment you will be using a corpus built over the source material of the [English Web Treebank](https://catalog.ldc.upenn.edu/ldc2012t13), consisting of approximately 16,000&nbsp;sentences with 254,000&nbsp;tokens. The corpus has been released by the [Universal Dependencies Project](http://universaldependencies.org).\n",
        "\n",
        "To make it easier to compare systems, the gold-standard data has been split into three parts: training, development (validation), and test. The following cell provides a function that can be used to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r1fmvsdu9ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    with open(path, encoding='utf-8') as fp:\n",
        "        result = []\n",
        "        for line in fp:\n",
        "            line = line.rstrip()\n",
        "            if len(line) == 0:\n",
        "                yield result\n",
        "                result = []\n",
        "            elif not line.startswith('#'):\n",
        "                columns = line.split()\n",
        "                if columns[0].isdigit():\n",
        "                    result.append((columns[1], columns[3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQkdQi-u9im",
        "colab_type": "text"
      },
      "source": [
        "The next cell loads the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJe3y9-Bu9in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "622150a5-f9c8-48c4-e61d-e755f6665213"
      },
      "source": [
        "train_data = list(read_data('en_ewt-ud-train.conllu'))\n",
        "print('Number of sentences in the training data: {}'.format(len(train_data)))\n",
        "\n",
        "dev_data = list(read_data('en_ewt-ud-dev.conllu'))\n",
        "print('Number of sentences in the development data: {}'.format(len(dev_data)))\n",
        "\n",
        "test_data = list(read_data('en_ewt-ud-test.conllu'))\n",
        "print('Number of sentences in the test data: {}'.format(len(test_data)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in the training data: 12543\n",
            "Number of sentences in the development data: 2002\n",
            "Number of sentences in the test data: 2077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVJreLa1u9iu",
        "colab_type": "text"
      },
      "source": [
        "From a Python perspective, each of the data sets is a list of what we shall refer to as *tagged sentences*. A tagged sentence, in turn, is a list of pairs $(w,t)$, where $w$ is a word token and $t$ is the word&rsquo;s POS tag. Here is an example from the training data to show you how this looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV4n_I-Qu9iv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "813c2477-541a-41aa-8285-40e4dacacaa9"
      },
      "source": [
        "train_data[42]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[', 'PUNCT'),\n",
              " ('This', 'DET'),\n",
              " ('killing', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('a', 'DET'),\n",
              " ('respected', 'ADJ'),\n",
              " ('cleric', 'NOUN'),\n",
              " ('will', 'AUX'),\n",
              " ('be', 'AUX'),\n",
              " ('causing', 'VERB'),\n",
              " ('us', 'PRON'),\n",
              " ('trouble', 'NOUN'),\n",
              " ('for', 'ADP'),\n",
              " ('years', 'NOUN'),\n",
              " ('to', 'PART'),\n",
              " ('come', 'VERB'),\n",
              " ('.', 'PUNCT'),\n",
              " (']', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFUvFhHy8Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "aaacdf67-a2ee-4709-cd73-5c1d0c6e6242"
      },
      "source": [
        "train_data[3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Two', 'NUM'),\n",
              " ('of', 'ADP'),\n",
              " ('them', 'PRON'),\n",
              " ('were', 'AUX'),\n",
              " ('being', 'AUX'),\n",
              " ('run', 'VERB'),\n",
              " ('by', 'ADP'),\n",
              " ('2', 'NUM'),\n",
              " ('officials', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('Ministry', 'PROPN'),\n",
              " ('of', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('Interior', 'PROPN'),\n",
              " ('!', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLGD5ej4u9i0",
        "colab_type": "text"
      },
      "source": [
        "You will see part-of-speech tags such as `VERB` for verb, `NOUN` for noun, and `ADV` for adverb. If you are interested in learning more about the tag set used in the gold-standard data, you can have a look at the documentation of the [Universal POS tags](http://universaldependencies.org/u/pos/all.html). However, you do not need to understand the meaning of the POS tags to solve this assignment; you can simply treat them as labels drawn from a finite set of alternatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fTyHKUcu9i1",
        "colab_type": "text"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3-w_vL-u9i2",
        "colab_type": "text"
      },
      "source": [
        "The proposed network architecture for your tagger is a sequential model with three layers, illustrated below: an embedding, a bidirectional LSTM, and a softmax layer. The embedding turns word indexes (integers representing words) into fixed-size dense vectors which are then fed into the bidirectional LSTM. The output of the LSTM at each position of the sentence is passed to a softmax layer which predicts the POS tag for the word at that position.\n",
        "\n",
        "![architecture.png](attachment:architecture.png)\n",
        "\n",
        "To implement the network architecture, you will use [Keras](https://keras.io/). Keras comes with an extensive online documentation, and reading the relevant parts of this documentation will be essential when working on this assignment. We suggest to start with the tutorial [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/). After that, you should have a look at some of the examples mentioned in that tutorial, and in particular the [Bidirectional LSTM](https://keras.io/examples/imdb_bidirectional_lstm/) example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9DmuN4tu9i3",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FemJ9HLu9i4",
        "colab_type": "text"
      },
      "source": [
        "The most widely-used evaluation measure for part-of-speech tagging is per-word accuracy, which is the percentage of words to which the tagger assigns the correct tag (according to the gold standard). This is one of the default metrics in Keras.\n",
        "\n",
        "One problem that you will encounter during evaluation is that the evaluation data contains words that you did not see (and did not add to your index) during training. The simplest solution to this problem is to introduce a special &lsquo;word&rsquo; `<unk>` and replace each unknown word with this pseudoword."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy5B9cy6u9i4",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Pre-process the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNKvccRsu9i6",
        "colab_type": "text"
      },
      "source": [
        "Before you can start to implement the network architecture as such, you will have to bring the tagged sentences from the gold-standard data into a form that can be used with the network. One important step in this is to map the words and tags (strings) to integers. Here is code that illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JW7IHLMu9i7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0087f22-7658-4de8-c265-9d99d1f1c9f9"
      },
      "source": [
        "word_to_index = {}\n",
        "tag_to_index = {}\n",
        "for tagged_sentence in train_data:\n",
        "    for word, tag in tagged_sentence:\n",
        "        if word not in word_to_index:\n",
        "            word_to_index[word] = len(word_to_index)\n",
        "        # if tag not in tag_to_index:\n",
        "        #     tag_to_index[tag] = tag\n",
        "\n",
        "print('Number of unique words in the training data: {}'.format(len(word_to_index)))\n",
        "print('Index of the word \"hates\": {}'.format(word_to_index['hates']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words in the training data: 19672\n",
            "Index of the word \"hates\": 4579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDzs0kNR1Kih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8bdd799-cb74-4201-fe09-448ffe761d23"
      },
      "source": [
        "word_to_index[\"child\"]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeV3i826u9jB",
        "colab_type": "text"
      },
      "source": [
        "Once you have indexes for the words and the tags, you can construct the input and the gold-standard output tensor required to train the network.\n",
        "\n",
        "### Constructing the input tensor\n",
        "\n",
        "The input tensor should be of shape $(N, n)$ where $N$ is the total number of sentences in the training data and $n$ is the length of the longest sentence. Note that Keras requires all sequences in an input tensor to have the same length, which means that you will have to pad all sequences to that length. You can use the helper function [`pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) for this, which by default will front-pad sequences with the value&nbsp;0. It is essential then that you do not use this special padding value as the index of actual words.\n",
        "\n",
        "### Constructing the target output tensor\n",
        "\n",
        "The target output tensor should be of shape $(N, n, T)$ where $T$ is the number of unique tags in the training data, plus one to cater for the special padding value. The additional dimension corresponds to the fact that the softmax layer of the network will output one $T$-dimensional vector for each position of an input sentence. To construct this vector, you can use the helper function [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3oHZgSuu9jC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "d57d6a35-ca7f-4387-98be-027d978fd9a3"
      },
      "source": [
        "# Define a help function to build index from a list of words or tags, each word / tag will have a unique number\n",
        "def build_index(strings, init=[]):\n",
        "    string_to_index = {s: i for i, s in enumerate(init)}\n",
        "    \n",
        "    # Loop over strings in 'strings'\n",
        "    for word_tag in strings:\n",
        "      if word_tag not in string_to_index.keys():\n",
        "        string_to_index[word_tag] = len(word_tag)\n",
        "\n",
        "        # Check if string exists in variable 'string_to_index', \n",
        "        # if string does not exist, add a new element to 'string_to_index': the current length of 'string_to_index'\n",
        "        \n",
        "    return string_to_index\n",
        "        \n",
        "# Convert all words and tags in train_data to lists, start with empty lists and use '.append()' \n",
        "# to add one word / tag at a time, similar to the cell below 'pre-process the data'\n",
        "words, tags = [], []\n",
        "\n",
        "\n",
        "# Call the help function you made, to build an index for words (word_to_index), and one index for tags (tag_to_index)\n",
        "\n",
        "\n",
        "# Check number of words and tags\n",
        "num_words = len(word_to_index)\n",
        "num_tags = len(tag_to_index)\n",
        "\n",
        "print(f'Number of unique words in the training data: {num_words}')\n",
        "print(f'Number of unique tags in the training_data: {num_tags}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9672646bff76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Check number of words and tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnum_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of unique words in the training data: {num_words}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tag_to_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agH7cHXe8sFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string_to_index = {s: i for i, s in enumerate(word_to_index)}\n",
        "  \n",
        "for word in string_to_index.keys\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Pw-rho8zyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fc8a92e-d3bd-4f23-a9a8-7e4e7d0d0f67"
      },
      "source": [
        "type(string_to_index)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMy108-lu9jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Make a function that converts the tagged sentences, word indices and tag indices to \n",
        "# X and Y, that can be used when training the RNN\n",
        "def encode(tagged_sentences, word_to_index, tag_to_index):\n",
        "    \n",
        "    # Start with empty lists that will contain all training examples and corresponding output\n",
        "    X, Y = [], []\n",
        "    \n",
        "    # Loop over tagged sentences\n",
        "    \n",
        "        # Make empty lists for current sentence\n",
        "        Xcurrent, Ycurrent = [], []\n",
        "    \n",
        "        # Loop over words and tags in current sentence\n",
        "        \n",
        "             # Append the index for the current word to Xcurrent, \n",
        "             # if the word does not exist in 'word_to_index', add the index for '<unk>' instead\n",
        "            \n",
        "             # Append the index for the current tag to Ycurrent\n",
        "                \n",
        "        # Append X with Xcurrent, and Y with Ycurrent\n",
        "    \n",
        "    # Pad the sequences, so that all have the same length\n",
        "    \n",
        "    # Convert labels to categorical, as you did in the CNN lab\n",
        "    \n",
        "    return X, Y\n",
        "    \n",
        "    \n",
        "    \n",
        "# Use your 'encode' function to create X and Y from train_data, word_to_index, tag_to_index\n",
        "\n",
        "\n",
        "# Print the shape of X and Y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4zAjq98u9jN",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Construct the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tY2Gtzeu9jO",
        "colab_type": "text"
      },
      "source": [
        "To implement the network architecture, you need to find and instantiate the relevant building blocks from the Keras library. Note that Keras layers support a large number of optional parameters; use the default values unless you have a good reason not to. Two mandatory parameters that you will have to specify are the dimensionality of the embedding and the dimensionality of the output of the LSTM layer. The following values are reasonable starting points, but do try a number of different settings.\n",
        "\n",
        "* dimensionality of the embedding: 100\n",
        "* dimensionality of the output of the bidirectional LSTM layer: 100\n",
        "\n",
        "You will also have to choose an appropriate loss function. For training we recommend the Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrPNY6iu9jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "# Import necessary layers\n",
        "from tensorflow.keras.layers import ?\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 100\n",
        "\n",
        "# Create model, similar to how it was done in the DNN and CNN labs\n",
        "model = Sequential()\n",
        "\n",
        "# The model should have an embedding layer, a bidirectional LSTM, and a dense softmax layer \n",
        "# (see the network architecture image)\n",
        "\n",
        "\n",
        "\n",
        "# Compile model\n",
        "\n",
        "\n",
        "# Print a summary of the model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvF-wWau9jT",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRWjRYou9jU",
        "colab_type": "text"
      },
      "source": [
        "The next step is to train the network. Use the following parameters:\n",
        "\n",
        "* number of epochs: 10\n",
        "* batch size: 32\n",
        "\n",
        "Training will print the average running loss on the training data after each minibatch. In addition to that, we ask you to also print the loss and accuracy on the development data after each epoch. You can do so by providing the `validation_data` argument to the `fit` method.\n",
        "\n",
        "Note that the `fit` method returns a [`History`](https://keras.io/callbacks/#history) object that contains useful information about the training. We will use that information in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI_i9zQ6u9jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the development (validation data) using the 'encode' function you created before\n",
        "\n",
        "\n",
        "# Train the model and save the history, as you did in the DNN and CNN labs, provide validation data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBlrRBu9ja",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: Identify a good model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMk-9Gf3u9jb",
        "colab_type": "text"
      },
      "source": [
        "The following code will plot the loss on the training data and the loss on the validation data after each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krSRV77qu9jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets define a help function for plotting the training results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(history):\n",
        "    \n",
        "    val_loss = history.history['val_loss']\n",
        "    acc = history.history['accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    \n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training','Validation'])\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.legend(['Training','Validation'])\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP5kA1OOu9jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_results(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szEmEqibu9jl",
        "colab_type": "text"
      },
      "source": [
        "Look at the plot and determine the epoch after which the model starts to overfit. Then, re-train your model using that many epochs and compute the accuracy of the tagger on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXzt9Ctu9jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the test_data using the 'encode' function you created before\n",
        "\n",
        "\n",
        "# Evaluate the model on test data, as you did in the DNN and CNN lab\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDB91F7au9jq",
        "colab_type": "text"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeYQ1btVu9jr",
        "colab_type": "text"
      },
      "source": [
        "Your notebook should include all your code, and should be runnable without further modification. It should also include answers to the following questions:\n",
        "\n",
        "How many epochs did you train the final model for?\n",
        "\n",
        "What accuracy did you achieve on the test data?\n",
        "\n",
        "What dimensionality of the embedding did you use for your best results?\n",
        "\n",
        "What dimensionality of the output of the bidirectional LSTM layer did you use for your best results?\n",
        "\n",
        "Instead of manually identifying a good model, and redoing the training to that number of epochs, how can you automatically stop the training when the validation performance does not improve anymore? Hint: see Lecture 2\n",
        "\n",
        "What did you find particularly surprising or hard?\n",
        "\n",
        "How do you calculate the number of parameters in the embedding layer? Hint: the calculation includes the vocabulary size and the embedding dimension\n",
        "\n",
        "How do you calculate the number of parameters in the bidirectional LSTM layer? Hint: A LSTM layer has 4 parts; cell, input gate, output gate, forget gate, each part contains two weight matrices and a bias vector. A bidirectional LSTM contains two LSTMs.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Long_short-term_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exiQoINJu9js",
        "colab_type": "text"
      },
      "source": [
        "*Insert your answers here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LDmhBotu9ju",
        "colab_type": "text"
      },
      "source": [
        "## Ethics in deep learning\n",
        "\n",
        "Now that you have watched the 5 lectures, and completed the 3 laborations in this course, what do you think is the most important ethical question related to deep learning? Motivate your answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxGRCk3Uu9jv",
        "colab_type": "text"
      },
      "source": [
        "*Insert your answer here.*"
      ]
    }
  ]
}